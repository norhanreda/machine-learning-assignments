{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this notebook, we will learn how to apply Logistic regression for predicting the cooling load requirements (Y2) of buildings as a function of building parameters (Xs).\n",
    "\n",
    "The attached dataset is taken from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency).\n",
    "\n",
    "To run this code, you will need the following python packages:\n",
    "* numpy\n",
    "* pandas\n",
    "* openpyxl\n",
    "* scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in d:\\anaconda\\lib\\site-packages (3.0.9)\n",
      "Requirement already satisfied: et-xmlfile in d:\\anaconda\\lib\\site-packages (from openpyxl) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the dataset using pandas\n",
    "df = pd.read_excel(\"Energy_Efficiency.xlsx\", engine = 'openpyxl')\n",
    "# Remove any unnamed columns (might occur due to difference in pandas readers)\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "# Remove any row with NaNs\n",
    "df = df.dropna(how='all')\n",
    "# Drop Y1 (as we only consider Y2 for classification)\n",
    "df = df.drop('Y1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we will split the dataframe into a training and testing splits with a 70% / 30% ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42) # Random is fixed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>334</th>\n",
       "      <td>0.62</td>\n",
       "      <td>808.5</td>\n",
       "      <td>367.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1</td>\n",
       "      <td>15.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>0.64</td>\n",
       "      <td>784.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>19.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>5</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>0.79</td>\n",
       "      <td>637.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1</td>\n",
       "      <td>46.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.79</td>\n",
       "      <td>637.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>30.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0.76</td>\n",
       "      <td>661.5</td>\n",
       "      <td>416.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1</td>\n",
       "      <td>33.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>0.86</td>\n",
       "      <td>588.0</td>\n",
       "      <td>294.0</td>\n",
       "      <td>147.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>27.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>0.71</td>\n",
       "      <td>710.5</td>\n",
       "      <td>269.5</td>\n",
       "      <td>220.50</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>5</td>\n",
       "      <td>14.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0.98</td>\n",
       "      <td>514.5</td>\n",
       "      <td>294.0</td>\n",
       "      <td>110.25</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.25</td>\n",
       "      <td>4</td>\n",
       "      <td>30.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>0.90</td>\n",
       "      <td>563.5</td>\n",
       "      <td>318.5</td>\n",
       "      <td>122.50</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2</td>\n",
       "      <td>29.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>537 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       X1     X2     X3      X4   X5  X6    X7  X8     Y2\n",
       "334  0.62  808.5  367.5  220.50  3.5   4  0.25   1  15.77\n",
       "139  0.64  784.0  343.0  220.50  3.5   5  0.10   2  19.30\n",
       "485  0.90  563.5  318.5  122.50  7.0   3  0.25   5  32.00\n",
       "547  0.79  637.0  343.0  147.00  7.0   5  0.40   1  46.94\n",
       "18   0.79  637.0  343.0  147.00  7.0   4  0.00   0  30.93\n",
       "..    ...    ...    ...     ...  ...  ..   ...  ..    ...\n",
       "71   0.76  661.5  416.5  122.50  7.0   5  0.10   1  33.67\n",
       "106  0.86  588.0  294.0  147.00  7.0   4  0.10   2  27.36\n",
       "270  0.71  710.5  269.5  220.50  3.5   4  0.10   5  14.26\n",
       "435  0.98  514.5  294.0  110.25  7.0   5  0.25   4  30.12\n",
       "102  0.90  563.5  318.5  122.50  7.0   4  0.10   2  29.36\n",
       "\n",
       "[537 rows x 9 columns]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>Y2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "      <td>537.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.760354</td>\n",
       "      <td>674.867784</td>\n",
       "      <td>318.636872</td>\n",
       "      <td>178.115456</td>\n",
       "      <td>5.201117</td>\n",
       "      <td>3.500931</td>\n",
       "      <td>0.235940</td>\n",
       "      <td>2.854749</td>\n",
       "      <td>24.287505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.104790</td>\n",
       "      <td>87.758133</td>\n",
       "      <td>43.619254</td>\n",
       "      <td>44.839207</td>\n",
       "      <td>1.750948</td>\n",
       "      <td>1.106502</td>\n",
       "      <td>0.134118</td>\n",
       "      <td>1.544532</td>\n",
       "      <td>9.505775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.620000</td>\n",
       "      <td>514.500000</td>\n",
       "      <td>245.000000</td>\n",
       "      <td>110.250000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.940000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.660000</td>\n",
       "      <td>612.500000</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>15.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.740000</td>\n",
       "      <td>686.000000</td>\n",
       "      <td>318.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>21.160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.820000</td>\n",
       "      <td>759.500000</td>\n",
       "      <td>343.000000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>32.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.980000</td>\n",
       "      <td>808.500000</td>\n",
       "      <td>416.500000</td>\n",
       "      <td>220.500000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>48.030000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               X1          X2          X3          X4          X5          X6  \\\n",
       "count  537.000000  537.000000  537.000000  537.000000  537.000000  537.000000   \n",
       "mean     0.760354  674.867784  318.636872  178.115456    5.201117    3.500931   \n",
       "std      0.104790   87.758133   43.619254   44.839207    1.750948    1.106502   \n",
       "min      0.620000  514.500000  245.000000  110.250000    3.500000    2.000000   \n",
       "25%      0.660000  612.500000  294.000000  147.000000    3.500000    3.000000   \n",
       "50%      0.740000  686.000000  318.500000  220.500000    3.500000    3.000000   \n",
       "75%      0.820000  759.500000  343.000000  220.500000    7.000000    4.000000   \n",
       "max      0.980000  808.500000  416.500000  220.500000    7.000000    5.000000   \n",
       "\n",
       "               X7          X8          Y2  \n",
       "count  537.000000  537.000000  537.000000  \n",
       "mean     0.235940    2.854749   24.287505  \n",
       "std      0.134118    1.544532    9.505775  \n",
       "min      0.000000    0.000000   10.940000  \n",
       "25%      0.100000    2.000000   15.500000  \n",
       "50%      0.250000    3.000000   21.160000  \n",
       "75%      0.400000    4.000000   32.920000  \n",
       "max      0.400000    5.000000   48.030000  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median value of the target: 21.16\n",
      "Percentage of 'high load' samples: 49.906890130353815 %\n"
     ]
    }
   ],
   "source": [
    "# Now we will extract the models input and targets from both the training and testing dataframes\n",
    "def extract_Xy(df):\n",
    "    df_numpy = df.to_numpy()\n",
    "    return df_numpy[:, :-1], df_numpy[:, -1]\n",
    "\n",
    "X_train, y_train = extract_Xy(df_train)\n",
    "X_test, y_test = extract_Xy(df_test)\n",
    "\n",
    "y_median = np.median(y_train)\n",
    "print(\"Median value of the target:\", y_median)\n",
    "\n",
    "# Since we will treat this as a classification task, we will assume that\n",
    "# the load is \"high\" (y = True) if its compressive ratio is higher than the median\n",
    "# otherwise, it is assumed to be \"low\" (y = False)\n",
    "y_train = y_train > y_median\n",
    "y_test = y_test > y_median\n",
    "\n",
    "# Now ~50% of the samples should be considered \"high\" and the rest are considered \"low\"\n",
    "print(f\"Percentage of 'high load' samples: {y_train.mean() * 100} %\")\n",
    "\n",
    "# Also, lets standardize the data since it improves the training process\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_mean)/(1e-8 + X_std)\n",
    "X_test = (X_test - X_mean)/(1e-8 + X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression via Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 20 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "model = LogisticRegression(random_state=0, penalty=\"none\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accurracy: 98.32402234636871%\n",
      "Testing Accurracy: 96.53679653679653%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train_predict = model.predict(X_train)\n",
    "print(f\"Training Accurracy: {accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = model.predict(X_test)\n",
    "print(f\"Testing Accurracy: {accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #TODO: Implement sigmoid (hint: use np.exp)\n",
    "    # pass\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(-1e2) = 3.7200759760208356e-44\n",
      "sigmoid(   0) = 0.5\n",
      "sigmoid(+1e2) = 1.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(f\"{sigmoid(-1e2) = }\") # This should be almost equal 0\n",
    "print(f\"{sigmoid(   0) = }\") # This should be exactly 0.5\n",
    "print(f\"{sigmoid(+1e2) = }\") # This should be almost equal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_accuracy_score(true, predicted):\n",
    "    #TODO: Implement an accuracy metric so that is can be used instead of Sklearn's accuracy score\n",
    "    #Note: both true and predicted will be boolean numpy array\n",
    "    # pass\n",
    "    return np.mean(true == predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "our_accuracy_score( np.array([True,  True]), np.array([True,  True]) ) = 1.0\n",
      "our_accuracy_score( np.array([True, False]), np.array([True,  True]) ) = 0.5\n",
      "our_accuracy_score( np.array([True, False]), np.array([True, False]) ) = 1.0\n",
      "our_accuracy_score( np.array([False, True]), np.array([True, False]) ) = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks\n",
    "print(f\"{our_accuracy_score( np.array([True,  True]), np.array([True,  True]) ) = }\") # Should be 1\n",
    "print(f\"{our_accuracy_score( np.array([True, False]), np.array([True,  True]) ) = }\") # Should be 0.5\n",
    "print(f\"{our_accuracy_score( np.array([True, False]), np.array([True, False]) ) = }\") # Should be 1\n",
    "print(f\"{our_accuracy_score( np.array([False, True]), np.array([True, False]) ) = }\") # Should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: You can only use numpy here. Do not use any premade algorithms (e.g. Scikit-Learn's Logistic Regression)\n",
    "class OurLogisticRegression:\n",
    "    def __init__(self, lr: int, epochs: int, probability_threshold: float = 0.5, random_state = None):\n",
    "        self.lr = lr # The learning rate\n",
    "        self.epochs = epochs # The number of training epochs\n",
    "        self.probability_threshold = probability_threshold # If the output of the sigmoid function is > probability_threshold, the prediction is considered to be positive (True)\n",
    "                                                           # otherwise, the prediction is considered to be negative (False)\n",
    "        self.random_state = random_state # The random state will be used set the random seed for the sake of reproducability\n",
    "    \n",
    "    def _prepare_input(self, X):\n",
    "        # Here, we add a new input with value 1 to each example. It will be multipled by the bias\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    def _prepare_target(self, y):\n",
    "        # Here, we convert True to +1 and False to -1\n",
    "        #TODO (Optional): You can modify your function if you wish to used other values for the positive and negative classes\n",
    "        return np.where(y, 1, -1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        # Here, we initialize the weights using a normally distributed random variable with a small standard deviation\n",
    "        self.w = np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        #TODO: Compute and return the gradient of the weights (self.w) wrt to the loss given the X and y arrays\n",
    "        # pass\n",
    "        return (-1/len(y))* np.dot(X.T,y*sigmoid(-y*np.dot(X,self.w)))\n",
    "\n",
    "    def _update(self, X, y):\n",
    "        #TODO: Implement this function to apply a single iteration on the weights \"self.w\"\n",
    "        #Hint: use self._gradient\n",
    "        # pass\n",
    "        self.w -= self.lr * self._gradient(X,y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state) # First, we set the seed\n",
    "        X = self._prepare_input(X) # Then we prepare the inputs\n",
    "        y = self._prepare_target(y) # and prepare the targets too\n",
    "        self._initialize(X.shape[1]) # and initialize the weights randomly\n",
    "        for _ in range(self.epochs): # Then we update the weights for a certain number of epochs\n",
    "            self._update(X, y)\n",
    "        return self # Return self to match the behavior of Scikit-Learn's LinearRegression fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        #TODO: Implement the rest of this function (Note: It should return a boolean array)\n",
    "        return sigmoid(np.dot(X,self.w)) > self.probability_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to tune the hyper parameters\n",
    "def validate(lr, epochs):\n",
    "    validation_size = 0.2 #TODO: Choose a size for the validation set as a ratio from the training data\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state=42)\n",
    "    # We will fit the model to only a subset of the training data and we will use the rest to evaluate the performance\n",
    "    our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_tr, y_tr)\n",
    "    # Then, we evaluate the peformance using the validation set\n",
    "    return our_accuracy_score(y_val, our_model.predict(X_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1 epochs, the accuracy reaches 100.0% using lr=0.5\n",
      "In 10 epochs, the accuracy reaches 100.0% using lr=0.5\n",
      "In 100 epochs, the accuracy reaches 100.0% using lr=0.5\n",
      "In 1000 epochs, the accuracy reaches 100.0% using lr=0.5\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5 #TODO: Choose a learning rate to use while testing different values for the number of epochs\n",
    "epochs_values = [1, 10, 100, 1000] #TODO: Choose a list of values for the number of epochs to test\n",
    "for epochs in epochs_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"In {epochs} epochs, the accuracy reaches {accuracy * 100}% using lr={lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lr=0.5, the accuracy reaches 100.0% in 10 epochs\n",
      "Using lr=0.1, the accuracy reaches 100.0% in 10 epochs\n",
      "Using lr=0.05, the accuracy reaches 100.0% in 10 epochs\n",
      "Using lr=0.01, the accuracy reaches 100.0% in 10 epochs\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 #TODO: Choose the number of epochs to use while testing different values for the learning rate\n",
    "lr_values = [0.5, 0.1, 0.05, 0.01] #TODO: Choose a list of values for the learning rate to test\n",
    "for lr in lr_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"Using lr={lr}, the accuracy reaches {accuracy * 100}% in {epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "#TODO: Select an appropriate learning rate and number of epochs\n",
    "lr = 0.5\n",
    "epochs = 1\n",
    "our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.32402234636871%\n",
      "Testing Accuracy: 96.53679653679653%\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = our_model.predict(X_train)\n",
    "print(f\"Training Accuracy: {our_accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = our_model.predict(X_test)\n",
    "print(f\"Testing Accuracy: {our_accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n*performance \\n\\n  -my implementation\\n     Training Accuracy: 98.32402234636871%\\n     Testing Accuracy: 96.53679653679653%\\n     \\n  -scikit-learn\\n     Training Accurracy: 98.32402234636871%\\n     Testing Accurracy: 96.53679653679653%\\n\\n\\n  my implementation performance is exactly the same as  scikit-learn performance  \\n\\n*training time \\n\\n  -my implementation\\n    CPU times: total: 0 ns\\n    Wall time: 0 ns\\n     \\n  -scikit-learn\\n     CPU times: total: 0 ns\\n     Wall time: 9.99 ms\\n\\n  my implementation  training time  is faster than  scikit-learn it may be due to the smallest number of epochs\\n\\n\\n'"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Write your conclusion about your implementation's performance and training time\n",
    "\n",
    "'''\n",
    "*performance \n",
    "\n",
    "  -my implementation\n",
    "     Training Accuracy: 98.32402234636871%\n",
    "     Testing Accuracy: 96.53679653679653%\n",
    "     \n",
    "  -scikit-learn\n",
    "     Training Accurracy: 98.32402234636871%\n",
    "     Testing Accurracy: 96.53679653679653%\n",
    "\n",
    "\n",
    "  my implementation performance is exactly the same as  scikit-learn performance  \n",
    "\n",
    "*training time \n",
    "\n",
    "  -my implementation\n",
    "    CPU times: total: 0 ns\n",
    "    Wall time: 1 ms\n",
    "     \n",
    "  -scikit-learn\n",
    "     CPU times: total: 46.9 ms\n",
    "     Wall time: 20 ms\n",
    "\n",
    "  my implementation  training time  is faster than  scikit-learn it may be due to the smallest number of epochs\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "As a bonus, you can implement and test the following:\n",
    "* Stochastic gradient descent\n",
    "* Termination conditions (e.g. The gradient check)\n",
    "  \n",
    "Write your conclusion about any results you calculate for your bonus implementations.\n",
    "\n",
    "**IMPORTANT**: Do not implement the bonus in the previous cells. You can copy and paste codes from the previous cells and continue your implementation below this cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic gradient descent\n",
    "# Termination conditions (e.g. The gradient check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OurLogisticRegression:\n",
    "    def __init__(self, lr: float, epochs: int, probability_threshold: float = 0.5, random_state=None):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.probability_threshold = probability_threshold\n",
    "        self.random_state = random_state\n",
    "    \n",
    "    def _prepare_input(self, X):\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    def _prepare_target(self, y):\n",
    "        return np.where(y, 1, -1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        self.w = np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _gradient(self, x, y):\n",
    "    \n",
    "        return -y*x*sigmoid(-y*np.dot(x,self.w))\n",
    "\n",
    "    def _update(self, x, y):\n",
    "        self.w -= self.lr * self._gradient(x, y)\n",
    "\n",
    "    def fit(self, X, y,threshold=1e-2):\n",
    "        np.random.seed(self.random_state)\n",
    "        X = self._prepare_input(X)\n",
    "        y = self._prepare_target(y)\n",
    "        self._initialize(X.shape[1])\n",
    "\n",
    "        previous_loss = float('inf')\n",
    "        for _ in range(self.epochs): # Then we update the weights for a certain number of epochs\n",
    "            randomized_indicies = np.random.permutation(X.shape[0])\n",
    "            X_permuted = X[randomized_indicies]\n",
    "            y_permuted = y[randomized_indicies]\n",
    "\n",
    "            for i in  range (X.shape[0]):\n",
    "\n",
    "                self._update(X_permuted[i],y_permuted[i])\n",
    "                gradient = self._gradient(X_permuted[i],y_permuted[i])\n",
    "                current_loss = np.linalg.norm(gradient)\n",
    "\n",
    "            if(np.abs(previous_loss-current_loss)) < threshold:\n",
    "                break\n",
    "            else :\n",
    "                previous_loss = current_loss\n",
    "\n",
    "\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        return sigmoid(np.dot(X, self.w)) > self.probability_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to tune the hyper parameters\n",
    "def validate(lr, epochs):\n",
    "    validation_size = 0.2 #TODO: Choose a size for the validation set as a ratio from the training data\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state=42)\n",
    "    # We will fit the model to only a subset of the training data and we will use the rest to evaluate the performance\n",
    "    our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_tr, y_tr)\n",
    "    # Then, we evaluate the peformance using the validation set\n",
    "    return our_accuracy_score(y_val, our_model.predict(X_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1 epochs, the accuracy reaches 100.0% using lr=0.5\n",
      "In 10 epochs, the accuracy reaches 100.0% using lr=0.5\n",
      "In 100 epochs, the accuracy reaches 100.0% using lr=0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 1000 epochs, the accuracy reaches 100.0% using lr=0.5\n"
     ]
    }
   ],
   "source": [
    "lr = 0.5 #TODO: Choose a learning rate to use while testing different values for the number of epochs\n",
    "epochs_values = [1, 10, 100, 1000] #TODO: Choose a list of values for the number of epochs to test\n",
    "for epochs in epochs_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"In {epochs} epochs, the accuracy reaches {accuracy * 100}% using lr={lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lr=0.5, the accuracy reaches 100.0% in 1 epochs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using lr=0.1, the accuracy reaches 100.0% in 1 epochs\n",
      "Using lr=0.05, the accuracy reaches 100.0% in 1 epochs\n",
      "Using lr=0.01, the accuracy reaches 100.0% in 1 epochs\n"
     ]
    }
   ],
   "source": [
    "epochs = 1 #TODO: Choose the number of epochs to use while testing different values for the learning rate\n",
    "lr_values = [0.5, 0.1, 0.05, 0.01] #TODO: Choose a list of values for the learning rate to test\n",
    "for lr in lr_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"Using lr={lr}, the accuracy reaches {accuracy * 100}% in {epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 31 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "#TODO: Select an appropriate learning rate and number of epochs\n",
    "lr = 0.5\n",
    "epochs = 1\n",
    "our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 98.32402234636871%\n",
      "Testing Accuracy: 96.53679653679653%\n"
     ]
    }
   ],
   "source": [
    "y_train_predict = our_model.predict(X_train)\n",
    "print(f\"Training Accuracy: {our_accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = our_model.predict(X_test)\n",
    "print(f\"Testing Accuracy: {our_accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n*performance \\n\\n  -Stochastic gradient descent\\n     Training Accuracy: 98.32402234636871%\\n     Testing Accuracy: 96.53679653679653%\\n     \\n  -gradient descent\\n     Training Accurracy: 98.32402234636871%\\n     Testing Accurracy: 96.53679653679653%\\n\\n\\n  it seems that Stochastic gradient descent and termination condition  performance is exactly the same as  gradient descent performance  \\n\\n*training time \\n\\n  -Stochastic gradient descent\\n      CPU times: total: 31.2 ms\\n      Wall time: 15 ms\\n     \\n  -gradient descent\\n      CPU times: total: 0 ns\\n      Wall time: 0 ns\\n\\n  Stochastic gradient descent and termination condition  training time  is larger than  gradient descent which make sense because the dataset here is\\n  very tiny and simple so the Stochastic gradient descent and termination consition would add more overhead here\\n  the Stochastic gradient descent would be better than the  gradient descent for large-scale problems due to its speed and memory efficiency but\\n  not here in our problem \\n\\n\\n'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Write your conclusion about your implementation's performance and training time\n",
    "\n",
    "'''\n",
    "*performance \n",
    "\n",
    "  -Stochastic gradient descent\n",
    "     Training Accuracy: 98.32402234636871%\n",
    "     Testing Accuracy: 96.53679653679653%\n",
    "     \n",
    "  -gradient descent\n",
    "     Training Accurracy: 98.32402234636871%\n",
    "     Testing Accurracy: 96.53679653679653%\n",
    "\n",
    "\n",
    "  it seems that Stochastic gradient descent and termination condition  performance is exactly the same as  gradient descent performance  \n",
    "\n",
    "*training time \n",
    "\n",
    "  -Stochastic gradient descent\n",
    "      CPU times: total: 31.2 ms\n",
    "      Wall time: 31 ms\n",
    "     \n",
    "  -gradient descent\n",
    "      CPU times: total: 0 ns\n",
    "      Wall time: 1 ms\n",
    "\n",
    "  Stochastic gradient descent and termination condition  training time  is larger than  gradient descent which make sense because the dataset here is\n",
    "  very tiny and simple so the Stochastic gradient descent and termination condition would add more overhead here\n",
    "  the Stochastic gradient descent would be better than the  gradient descent for large-scale problems due to its speed and memory efficiency but\n",
    "  not here in our problem \n",
    "\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd780a10ad03a506e232ec29f104692e8d999a77309c0fc915217df500c72051"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
