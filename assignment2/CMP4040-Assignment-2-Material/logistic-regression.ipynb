{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "In this notebook, we will learn how to apply Logistic regression for predicting the cooling load requirements (Y2) of buildings as a function of building parameters (Xs).\n",
    "\n",
    "The attached dataset is taken from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Energy+efficiency).\n",
    "\n",
    "To run this code, you will need the following python packages:\n",
    "* numpy\n",
    "* pandas\n",
    "* openpyxl\n",
    "* scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we load the dataset using pandas\n",
    "df = pd.read_excel(\"Energy_Efficiency.xlsx\", engine = 'openpyxl')\n",
    "# Remove any unnamed columns (might occur due to difference in pandas readers)\n",
    "df = df.loc[:, ~df.columns.str.contains('^Unnamed')]\n",
    "# Remove any row with NaNs\n",
    "df = df.dropna(how='all')\n",
    "# Drop Y1 (as we only consider Y2 for classification)\n",
    "df = df.drop('Y1', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next, we will split the dataframe into a training and testing splits with a 70% / 30% ratio\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.3, random_state=42) # Random is fixed for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will extract the models input and targets from both the training and testing dataframes\n",
    "def extract_Xy(df):\n",
    "    df_numpy = df.to_numpy()\n",
    "    return df_numpy[:, :-1], df_numpy[:, -1]\n",
    "\n",
    "X_train, y_train = extract_Xy(df_train)\n",
    "X_test, y_test = extract_Xy(df_test)\n",
    "\n",
    "y_median = np.median(y_train)\n",
    "print(\"Median value of the target:\", y_median)\n",
    "\n",
    "# Since we will treat this as a classification task, we will assume that\n",
    "# the load is \"high\" (y = True) if its compressive ratio is higher than the median\n",
    "# otherwise, it is assumed to be \"low\" (y = False)\n",
    "y_train = y_train > y_median\n",
    "y_test = y_test > y_median\n",
    "\n",
    "# Now ~50% of the samples should be considered \"high\" and the rest are considered \"low\"\n",
    "print(f\"Percentage of 'high load' samples: {y_train.mean() * 100} %\")\n",
    "\n",
    "# Also, lets standardize the data since it improves the training process\n",
    "X_mean = X_train.mean(axis=0)\n",
    "X_std = X_train.std(axis=0)\n",
    "X_train = (X_train - X_mean)/(1e-8 + X_std)\n",
    "X_test = (X_test - X_mean)/(1e-8 + X_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression via Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "model = LogisticRegression(random_state=0, penalty=\"none\").fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_train_predict = model.predict(X_train)\n",
    "print(f\"Training Accurracy: {accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = model.predict(X_test)\n",
    "print(f\"Testing Accurracy: {accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    #TODO: Implement sigmoid (hint: use np.exp)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "print(f\"{sigmoid(-1e2) = }\") # This should be almost equal 0\n",
    "print(f\"{sigmoid(   0) = }\") # This should be exactly 0.5\n",
    "print(f\"{sigmoid(+1e2) = }\") # This should be almost equal 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_accuracy_score(true, predicted):\n",
    "    #TODO: Implement an accuracy metric so that is can be used instead of Sklearn's accuracy score\n",
    "    #Note: both true and predicted will be boolean numpy array\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "print(f\"{our_accuracy_score( np.array([True,  True]), np.array([True,  True]) ) = }\") # Should be 1\n",
    "print(f\"{our_accuracy_score( np.array([True, False]), np.array([True,  True]) ) = }\") # Should be 0.5\n",
    "print(f\"{our_accuracy_score( np.array([True, False]), np.array([True, False]) ) = }\") # Should be 1\n",
    "print(f\"{our_accuracy_score( np.array([False, True]), np.array([True, False]) ) = }\") # Should be 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTANT: You can only use numpy here. Do not use any premade algorithms (e.g. Scikit-Learn's Logistic Regression)\n",
    "class OurLogisticRegression:\n",
    "    def __init__(self, lr: int, epochs: int, probability_threshold: float = 0.5, random_state = None):\n",
    "        self.lr = lr # The learning rate\n",
    "        self.epochs = epochs # The number of training epochs\n",
    "        self.probability_threshold = probability_threshold # If the output of the sigmoid function is > probability_threshold, the prediction is considered to be positive (True)\n",
    "                                                           # otherwise, the prediction is considered to be negative (False)\n",
    "        self.random_state = random_state # The random state will be used set the random seed for the sake of reproducability\n",
    "    \n",
    "    def _prepare_input(self, X):\n",
    "        # Here, we add a new input with value 1 to each example. It will be multipled by the bias\n",
    "        ones = np.ones((X.shape[0], 1), dtype=X.dtype)\n",
    "        return np.concatenate((ones, X), axis=1)\n",
    "    \n",
    "    def _prepare_target(self, y):\n",
    "        # Here, we convert True to +1 and False to -1\n",
    "        #TODO (Optional): You can modify your function if you wish to used other values for the positive and negative classes\n",
    "        return np.where(y, 1, -1)\n",
    "\n",
    "    def _initialize(self, num_weights: int, stdev: float = 0.01):\n",
    "        # Here, we initialize the weights using a normally distributed random variable with a small standard deviation\n",
    "        self.w = np.random.randn(num_weights) * stdev\n",
    "\n",
    "    def _gradient(self, X, y):\n",
    "        #TODO: Compute and return the gradient of the weights (self.w) wrt to the loss given the X and y arrays\n",
    "        pass\n",
    "\n",
    "    def _update(self, X, y):\n",
    "        #TODO: Implement this function to apply a single iteration on the weights \"self.w\"\n",
    "        #Hint: use self._gradient\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state) # First, we set the seed\n",
    "        X = self._prepare_input(X) # Then we prepare the inputs\n",
    "        y = self._prepare_target(y) # and prepare the targets too\n",
    "        self._initialize(X.shape[1]) # and initialize the weights randomly\n",
    "        for _ in range(self.epochs): # Then we update the weights for a certain number of epochs\n",
    "            self._update(X, y)\n",
    "        return self # Return self to match the behavior of Scikit-Learn's LinearRegression fit()\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self._prepare_input(X)\n",
    "        #TODO: Implement the rest of this function (Note: It should return a boolean array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use this function to tune the hyper parameters\n",
    "def validate(lr, epochs):\n",
    "    validation_size = None #TODO: Choose a size for the validation set as a ratio from the training data\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=validation_size, random_state=42)\n",
    "    # We will fit the model to only a subset of the training data and we will use the rest to evaluate the performance\n",
    "    our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_tr, y_tr)\n",
    "    # Then, we evaluate the peformance using the validation set\n",
    "    return our_accuracy_score(y_val, our_model.predict(X_val)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = None #TODO: Choose a learning rate to use while testing different values for the number of epochs\n",
    "epochs_values = [] #TODO: Choose a list of values for the number of epochs to test\n",
    "for epochs in epochs_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"In {epochs} epochs, the accuracy reaches {accuracy * 100}% using lr={lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = None #TODO: Choose the number of epochs to use while testing different values for the learning rate\n",
    "lr_values = [] #TODO: Choose a list of values for the learning rate to test\n",
    "for lr in lr_values:\n",
    "    accuracy = validate(lr, epochs)\n",
    "    print(f\"Using lr={lr}, the accuracy reaches {accuracy * 100}% in {epochs} epochs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# We use time to compute the training time of our model\n",
    "#TODO: Select an appropriate learning rate and number of epochs\n",
    "lr = None\n",
    "epochs = None\n",
    "our_model = OurLogisticRegression(lr=lr, epochs=epochs, random_state=0).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predict = our_model.predict(X_train)\n",
    "print(f\"Training Accuracy: {our_accuracy_score(y_train, y_train_predict) * 100}%\")\n",
    "y_test_predict = our_model.predict(X_test)\n",
    "print(f\"Testing Accuracy: {our_accuracy_score(y_test, y_test_predict) * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Write your conclusion about your implementation's performance and training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "As a bonus, you can implement and test the following:\n",
    "* Stochastic gradient descent\n",
    "* Termination conditions (e.g. The gradient check)\n",
    "  \n",
    "Write your conclusion about any results you calculate for your bonus implementations.\n",
    "\n",
    "**IMPORTANT**: Do not implement the bonus in the previous cells. You can copy and paste codes from the previous cells and continue your implementation below this cell."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd780a10ad03a506e232ec29f104692e8d999a77309c0fc915217df500c72051"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
